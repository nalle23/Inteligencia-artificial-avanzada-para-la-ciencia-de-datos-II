{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo 2 - 8. Dense + Dropout + Batch Normalization\n",
    "\n",
    "Nallely Lizbeth Serna Rivera    A00833111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   StudentID  Age  Gender  Ethnicity  ParentalEducation  StudyTimeWeekly  \\\n",
      "0       1001   17       1          0                  2        19.833723   \n",
      "1       1002   18       0          0                  1        15.408756   \n",
      "2       1003   15       0          2                  3         4.210570   \n",
      "3       1004   17       1          0                  3        10.028829   \n",
      "4       1005   17       1          0                  2         4.672495   \n",
      "\n",
      "   Absences  Tutoring  ParentalSupport  Extracurricular  Sports  Music  \\\n",
      "0         7         1                2                0       0      1   \n",
      "1         0         0                1                0       0      0   \n",
      "2        26         0                2                0       0      0   \n",
      "3        14         0                3                1       0      0   \n",
      "4        17         1                3                0       0      0   \n",
      "\n",
      "   Volunteering       GPA  GradeClass  \n",
      "0             0  2.929196         2.0  \n",
      "1             0  3.042915         1.0  \n",
      "2             0  0.112602         4.0  \n",
      "3             0  2.054218         3.0  \n",
      "4             0  1.288061         4.0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2392 entries, 0 to 2391\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   StudentID          2392 non-null   int64  \n",
      " 1   Age                2392 non-null   int64  \n",
      " 2   Gender             2392 non-null   int64  \n",
      " 3   Ethnicity          2392 non-null   int64  \n",
      " 4   ParentalEducation  2392 non-null   int64  \n",
      " 5   StudyTimeWeekly    2392 non-null   float64\n",
      " 6   Absences           2392 non-null   int64  \n",
      " 7   Tutoring           2392 non-null   int64  \n",
      " 8   ParentalSupport    2392 non-null   int64  \n",
      " 9   Extracurricular    2392 non-null   int64  \n",
      " 10  Sports             2392 non-null   int64  \n",
      " 11  Music              2392 non-null   int64  \n",
      " 12  Volunteering       2392 non-null   int64  \n",
      " 13  GPA                2392 non-null   float64\n",
      " 14  GradeClass         2392 non-null   float64\n",
      "dtypes: float64(3), int64(12)\n",
      "memory usage: 280.4 KB\n",
      "None\n",
      "         StudentID          Age       Gender    Ethnicity  ParentalEducation  \\\n",
      "count  2392.000000  2392.000000  2392.000000  2392.000000        2392.000000   \n",
      "mean   2196.500000    16.468645     0.510870     0.877508           1.746237   \n",
      "std     690.655244     1.123798     0.499986     1.028476           1.000411   \n",
      "min    1001.000000    15.000000     0.000000     0.000000           0.000000   \n",
      "25%    1598.750000    15.000000     0.000000     0.000000           1.000000   \n",
      "50%    2196.500000    16.000000     1.000000     0.000000           2.000000   \n",
      "75%    2794.250000    17.000000     1.000000     2.000000           2.000000   \n",
      "max    3392.000000    18.000000     1.000000     3.000000           4.000000   \n",
      "\n",
      "       StudyTimeWeekly     Absences     Tutoring  ParentalSupport  \\\n",
      "count      2392.000000  2392.000000  2392.000000      2392.000000   \n",
      "mean          9.771992    14.541388     0.301421         2.122074   \n",
      "std           5.652774     8.467417     0.458971         1.122813   \n",
      "min           0.001057     0.000000     0.000000         0.000000   \n",
      "25%           5.043079     7.000000     0.000000         1.000000   \n",
      "50%           9.705363    15.000000     0.000000         2.000000   \n",
      "75%          14.408410    22.000000     1.000000         3.000000   \n",
      "max          19.978094    29.000000     1.000000         4.000000   \n",
      "\n",
      "       Extracurricular       Sports        Music  Volunteering          GPA  \\\n",
      "count      2392.000000  2392.000000  2392.000000   2392.000000  2392.000000   \n",
      "mean          0.383361     0.303512     0.196906      0.157191     1.906186   \n",
      "std           0.486307     0.459870     0.397744      0.364057     0.915156   \n",
      "min           0.000000     0.000000     0.000000      0.000000     0.000000   \n",
      "25%           0.000000     0.000000     0.000000      0.000000     1.174803   \n",
      "50%           0.000000     0.000000     0.000000      0.000000     1.893393   \n",
      "75%           1.000000     1.000000     0.000000      0.000000     2.622216   \n",
      "max           1.000000     1.000000     1.000000      1.000000     4.000000   \n",
      "\n",
      "        GradeClass  \n",
      "count  2392.000000  \n",
      "mean      2.983696  \n",
      "std       1.233908  \n",
      "min       0.000000  \n",
      "25%       2.000000  \n",
      "50%       4.000000  \n",
      "75%       4.000000  \n",
      "max       4.000000  \n"
     ]
    }
   ],
   "source": [
    "# Cargar los datos\n",
    "df = pd.read_csv('student_performance_data _.csv')\n",
    "\n",
    "# Exploración de los datos\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "\n",
    "# Separar características y la variable objetivo (GPA)\n",
    "X = df.drop(columns=['GPA', 'StudentID'])  # Eliminar 'StudentID' ya que no aporta valor predictivo\n",
    "y = df['GPA']\n",
    "\n",
    "# Escalar los datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Dividir en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Función para entrenar y evaluar el modelo\n",
    "def train_and_evaluate(model):\n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Entrenar el modelo\n",
    "    history = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), batch_size=32)\n",
    "    \n",
    "    # Evaluar el modelo\n",
    "    test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "    return test_mae, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "60/60 [==============================] - 2s 3ms/step - loss: 1.8729 - mae: 1.0983 - val_loss: 0.6240 - val_mae: 0.6586\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.3090 - mae: 0.4470 - val_loss: 0.1838 - val_mae: 0.3481\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1589 - mae: 0.3170 - val_loss: 0.1352 - val_mae: 0.2961\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1249 - mae: 0.2808 - val_loss: 0.1133 - val_mae: 0.2709\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1033 - mae: 0.2567 - val_loss: 0.0969 - val_mae: 0.2518\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0877 - mae: 0.2376 - val_loss: 0.0854 - val_mae: 0.2372\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0768 - mae: 0.2229 - val_loss: 0.0772 - val_mae: 0.2250\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0691 - mae: 0.2124 - val_loss: 0.0717 - val_mae: 0.2171\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0635 - mae: 0.2038 - val_loss: 0.0671 - val_mae: 0.2093\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0592 - mae: 0.1966 - val_loss: 0.0626 - val_mae: 0.2017\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0557 - mae: 0.1908 - val_loss: 0.0599 - val_mae: 0.1976\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0528 - mae: 0.1860 - val_loss: 0.0575 - val_mae: 0.1937\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0505 - mae: 0.1819 - val_loss: 0.0561 - val_mae: 0.1911\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0489 - mae: 0.1793 - val_loss: 0.0538 - val_mae: 0.1879\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0464 - mae: 0.1744 - val_loss: 0.0523 - val_mae: 0.1832\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0451 - mae: 0.1723 - val_loss: 0.0507 - val_mae: 0.1811\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0437 - mae: 0.1700 - val_loss: 0.0498 - val_mae: 0.1798\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.1663 - val_loss: 0.0491 - val_mae: 0.1766\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0413 - mae: 0.1652 - val_loss: 0.0471 - val_mae: 0.1749\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0399 - mae: 0.1620 - val_loss: 0.0465 - val_mae: 0.1735\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0388 - mae: 0.1599 - val_loss: 0.0470 - val_mae: 0.1739\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0377 - mae: 0.1571 - val_loss: 0.0458 - val_mae: 0.1725\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0374 - mae: 0.1567 - val_loss: 0.0453 - val_mae: 0.1707\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0361 - mae: 0.1538 - val_loss: 0.0444 - val_mae: 0.1692\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0354 - mae: 0.1519 - val_loss: 0.0441 - val_mae: 0.1687\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0349 - mae: 0.1509 - val_loss: 0.0436 - val_mae: 0.1675\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0341 - mae: 0.1496 - val_loss: 0.0430 - val_mae: 0.1676\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0336 - mae: 0.1482 - val_loss: 0.0437 - val_mae: 0.1684\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0330 - mae: 0.1466 - val_loss: 0.0433 - val_mae: 0.1681\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0325 - mae: 0.1453 - val_loss: 0.0420 - val_mae: 0.1629\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0318 - mae: 0.1436 - val_loss: 0.0422 - val_mae: 0.1647\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0316 - mae: 0.1429 - val_loss: 0.0424 - val_mae: 0.1647\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0312 - mae: 0.1422 - val_loss: 0.0420 - val_mae: 0.1649\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0305 - mae: 0.1407 - val_loss: 0.0412 - val_mae: 0.1623\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0305 - mae: 0.1399 - val_loss: 0.0418 - val_mae: 0.1623\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0300 - mae: 0.1389 - val_loss: 0.0417 - val_mae: 0.1637\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0297 - mae: 0.1379 - val_loss: 0.0418 - val_mae: 0.1634\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0292 - mae: 0.1372 - val_loss: 0.0411 - val_mae: 0.1622\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0295 - mae: 0.1374 - val_loss: 0.0422 - val_mae: 0.1629\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0288 - mae: 0.1358 - val_loss: 0.0418 - val_mae: 0.1638\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0283 - mae: 0.1342 - val_loss: 0.0413 - val_mae: 0.1614\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0283 - mae: 0.1343 - val_loss: 0.0417 - val_mae: 0.1623\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0280 - mae: 0.1338 - val_loss: 0.0410 - val_mae: 0.1607\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0278 - mae: 0.1335 - val_loss: 0.0411 - val_mae: 0.1609\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0273 - mae: 0.1318 - val_loss: 0.0409 - val_mae: 0.1603\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0271 - mae: 0.1315 - val_loss: 0.0413 - val_mae: 0.1625\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0272 - mae: 0.1309 - val_loss: 0.0414 - val_mae: 0.1623\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0268 - mae: 0.1302 - val_loss: 0.0409 - val_mae: 0.1598\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0261 - mae: 0.1287 - val_loss: 0.0409 - val_mae: 0.1605\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0262 - mae: 0.1288 - val_loss: 0.0416 - val_mae: 0.1619\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0416 - mae: 0.1619\n"
     ]
    }
   ],
   "source": [
    "# Experimento 1: Modelo con una capa oculta\n",
    "model_1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "mae_1, history_1 = train_and_evaluate(model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "60/60 [==============================] - 1s 4ms/step - loss: 1.9622 - mae: 1.1421 - val_loss: 0.4160 - val_mae: 0.5161\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2249 - mae: 0.3742 - val_loss: 0.1613 - val_mae: 0.3177\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1194 - mae: 0.2748 - val_loss: 0.1064 - val_mae: 0.2568\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0825 - mae: 0.2303 - val_loss: 0.0827 - val_mae: 0.2258\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0658 - mae: 0.2066 - val_loss: 0.0758 - val_mae: 0.2152\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0566 - mae: 0.1912 - val_loss: 0.0695 - val_mae: 0.2063\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0511 - mae: 0.1818 - val_loss: 0.0643 - val_mae: 0.1980\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0465 - mae: 0.1737 - val_loss: 0.0634 - val_mae: 0.1968\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0433 - mae: 0.1674 - val_loss: 0.0613 - val_mae: 0.1931\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0414 - mae: 0.1644 - val_loss: 0.0605 - val_mae: 0.1909\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0396 - mae: 0.1598 - val_loss: 0.0600 - val_mae: 0.1895\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0379 - mae: 0.1573 - val_loss: 0.0591 - val_mae: 0.1886\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0360 - mae: 0.1523 - val_loss: 0.0587 - val_mae: 0.1875\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0356 - mae: 0.1525 - val_loss: 0.0580 - val_mae: 0.1859\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0341 - mae: 0.1493 - val_loss: 0.0589 - val_mae: 0.1901\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0329 - mae: 0.1461 - val_loss: 0.0566 - val_mae: 0.1854\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0321 - mae: 0.1441 - val_loss: 0.0568 - val_mae: 0.1843\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0310 - mae: 0.1411 - val_loss: 0.0571 - val_mae: 0.1847\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0310 - mae: 0.1417 - val_loss: 0.0565 - val_mae: 0.1841\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0303 - mae: 0.1392 - val_loss: 0.0559 - val_mae: 0.1835\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0295 - mae: 0.1371 - val_loss: 0.0562 - val_mae: 0.1833\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0287 - mae: 0.1355 - val_loss: 0.0563 - val_mae: 0.1849\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0283 - mae: 0.1342 - val_loss: 0.0547 - val_mae: 0.1816\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0279 - mae: 0.1336 - val_loss: 0.0532 - val_mae: 0.1801\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0271 - mae: 0.1315 - val_loss: 0.0539 - val_mae: 0.1795\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0265 - mae: 0.1290 - val_loss: 0.0547 - val_mae: 0.1826\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0263 - mae: 0.1290 - val_loss: 0.0552 - val_mae: 0.1823\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0253 - mae: 0.1268 - val_loss: 0.0543 - val_mae: 0.1829\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0254 - mae: 0.1265 - val_loss: 0.0555 - val_mae: 0.1827\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0249 - mae: 0.1252 - val_loss: 0.0550 - val_mae: 0.1836\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0240 - mae: 0.1223 - val_loss: 0.0533 - val_mae: 0.1802\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0251 - mae: 0.1254 - val_loss: 0.0562 - val_mae: 0.1875\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0245 - mae: 0.1237 - val_loss: 0.0530 - val_mae: 0.1790\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0231 - mae: 0.1201 - val_loss: 0.0544 - val_mae: 0.1809\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0227 - mae: 0.1192 - val_loss: 0.0552 - val_mae: 0.1844\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0222 - mae: 0.1176 - val_loss: 0.0530 - val_mae: 0.1785\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0223 - mae: 0.1178 - val_loss: 0.0525 - val_mae: 0.1778\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0220 - mae: 0.1166 - val_loss: 0.0530 - val_mae: 0.1792\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0222 - mae: 0.1183 - val_loss: 0.0538 - val_mae: 0.1815\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0215 - mae: 0.1152 - val_loss: 0.0550 - val_mae: 0.1822\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0212 - mae: 0.1147 - val_loss: 0.0529 - val_mae: 0.1782\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0207 - mae: 0.1129 - val_loss: 0.0522 - val_mae: 0.1782\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0210 - mae: 0.1146 - val_loss: 0.0541 - val_mae: 0.1811\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0201 - mae: 0.1108 - val_loss: 0.0546 - val_mae: 0.1828\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0197 - mae: 0.1098 - val_loss: 0.0539 - val_mae: 0.1815\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0195 - mae: 0.1097 - val_loss: 0.0513 - val_mae: 0.1772\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0187 - mae: 0.1075 - val_loss: 0.0545 - val_mae: 0.1819\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0189 - mae: 0.1081 - val_loss: 0.0542 - val_mae: 0.1805\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0187 - mae: 0.1068 - val_loss: 0.0566 - val_mae: 0.1855\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0184 - mae: 0.1066 - val_loss: 0.0538 - val_mae: 0.1795\n",
      "15/15 [==============================] - 0s 969us/step - loss: 0.0538 - mae: 0.1795\n"
     ]
    }
   ],
   "source": [
    "# Experimento 2: Modelo con tres capas ocultas\n",
    "model_2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "mae_2, history_2 = train_and_evaluate(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "60/60 [==============================] - 5s 63ms/step - loss: 2.2558 - mae: 1.2212 - val_loss: 0.5236 - val_mae: 0.6127\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9895 - mae: 0.7821 - val_loss: 0.4112 - val_mae: 0.5569\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.7894 - mae: 0.7003 - val_loss: 0.2970 - val_mae: 0.4725\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.6529 - mae: 0.6396 - val_loss: 0.2356 - val_mae: 0.4229\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.6029 - mae: 0.6080 - val_loss: 0.2508 - val_mae: 0.4381\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.5402 - mae: 0.5754 - val_loss: 0.1754 - val_mae: 0.3627\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.5425 - mae: 0.5718 - val_loss: 0.1429 - val_mae: 0.3231\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.4603 - mae: 0.5267 - val_loss: 0.1727 - val_mae: 0.3631\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.4777 - mae: 0.5314 - val_loss: 0.1543 - val_mae: 0.3411\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.4451 - mae: 0.5121 - val_loss: 0.2035 - val_mae: 0.3948\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.4168 - mae: 0.4965 - val_loss: 0.1335 - val_mae: 0.3169\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.3698 - mae: 0.4679 - val_loss: 0.1168 - val_mae: 0.2949\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.3976 - mae: 0.4802 - val_loss: 0.0913 - val_mae: 0.2538\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.3777 - mae: 0.4765 - val_loss: 0.1031 - val_mae: 0.2738\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.3530 - mae: 0.4599 - val_loss: 0.1078 - val_mae: 0.2793\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.3538 - mae: 0.4493 - val_loss: 0.1445 - val_mae: 0.3268\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.3330 - mae: 0.4359 - val_loss: 0.0795 - val_mae: 0.2373\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.3273 - mae: 0.4432 - val_loss: 0.0881 - val_mae: 0.2494\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2993 - mae: 0.4202 - val_loss: 0.1094 - val_mae: 0.2820\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.3069 - mae: 0.4245 - val_loss: 0.1032 - val_mae: 0.2727\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2991 - mae: 0.4129 - val_loss: 0.0950 - val_mae: 0.2602\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.3007 - mae: 0.4102 - val_loss: 0.0745 - val_mae: 0.2270\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2566 - mae: 0.3831 - val_loss: 0.0810 - val_mae: 0.2401\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2658 - mae: 0.3890 - val_loss: 0.0930 - val_mae: 0.2573\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2583 - mae: 0.3869 - val_loss: 0.0750 - val_mae: 0.2302\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2415 - mae: 0.3730 - val_loss: 0.0585 - val_mae: 0.2001\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2439 - mae: 0.3747 - val_loss: 0.0850 - val_mae: 0.2453\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2465 - mae: 0.3747 - val_loss: 0.0710 - val_mae: 0.2229\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2237 - mae: 0.3597 - val_loss: 0.0707 - val_mae: 0.2233\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2377 - mae: 0.3722 - val_loss: 0.0847 - val_mae: 0.2433\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2241 - mae: 0.3550 - val_loss: 0.0718 - val_mae: 0.2237\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2179 - mae: 0.3550 - val_loss: 0.0683 - val_mae: 0.2170\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1984 - mae: 0.3378 - val_loss: 0.0755 - val_mae: 0.2297\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1953 - mae: 0.3362 - val_loss: 0.0709 - val_mae: 0.2219\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1957 - mae: 0.3306 - val_loss: 0.0466 - val_mae: 0.1733\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1887 - mae: 0.3278 - val_loss: 0.0557 - val_mae: 0.1921\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1972 - mae: 0.3361 - val_loss: 0.0777 - val_mae: 0.2300\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1909 - mae: 0.3332 - val_loss: 0.0632 - val_mae: 0.2053\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1998 - mae: 0.3344 - val_loss: 0.0585 - val_mae: 0.1980\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1733 - mae: 0.3171 - val_loss: 0.0624 - val_mae: 0.2041\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1702 - mae: 0.3169 - val_loss: 0.0534 - val_mae: 0.1873\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1776 - mae: 0.3200 - val_loss: 0.0548 - val_mae: 0.1904\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1666 - mae: 0.3074 - val_loss: 0.0693 - val_mae: 0.2139\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1720 - mae: 0.3126 - val_loss: 0.0576 - val_mae: 0.1913\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1647 - mae: 0.3045 - val_loss: 0.0731 - val_mae: 0.2198\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1554 - mae: 0.3008 - val_loss: 0.0588 - val_mae: 0.1950\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1661 - mae: 0.3042 - val_loss: 0.0643 - val_mae: 0.2039\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1587 - mae: 0.3022 - val_loss: 0.0545 - val_mae: 0.1870\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1554 - mae: 0.2997 - val_loss: 0.0623 - val_mae: 0.1995\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1628 - mae: 0.3044 - val_loss: 0.0727 - val_mae: 0.2189\n",
      "15/15 [==============================] - 0s 933us/step - loss: 0.0727 - mae: 0.2189\n"
     ]
    }
   ],
   "source": [
    "# Experimento 3: Añadir Dropout después de cada capa oculta\n",
    "model_3 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "mae_3, history_3 = train_and_evaluate(model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "60/60 [==============================] - 6s 4ms/step - loss: 4.8493 - mae: 1.8715 - val_loss: 3.5863 - val_mae: 1.7081\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 3.4317 - mae: 1.5812 - val_loss: 2.6257 - val_mae: 1.4651\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 2.4876 - mae: 1.3245 - val_loss: 1.7388 - val_mae: 1.1935\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 1.8143 - mae: 1.1126 - val_loss: 1.0761 - val_mae: 0.9277\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 1.3192 - mae: 0.9315 - val_loss: 0.5983 - val_mae: 0.6818\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.9857 - mae: 0.8012 - val_loss: 0.3470 - val_mae: 0.5085\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.8318 - mae: 0.7286 - val_loss: 0.2592 - val_mae: 0.4385\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.7038 - mae: 0.6646 - val_loss: 0.1911 - val_mae: 0.3676\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6175 - mae: 0.6209 - val_loss: 0.1669 - val_mae: 0.3436\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.5077 - mae: 0.5672 - val_loss: 0.1427 - val_mae: 0.3137\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.5138 - mae: 0.5669 - val_loss: 0.1328 - val_mae: 0.3039\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.4400 - mae: 0.5264 - val_loss: 0.1068 - val_mae: 0.2683\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.3901 - mae: 0.4954 - val_loss: 0.1023 - val_mae: 0.2643\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.3808 - mae: 0.4901 - val_loss: 0.0903 - val_mae: 0.2469\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.3541 - mae: 0.4748 - val_loss: 0.0862 - val_mae: 0.2395\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.3223 - mae: 0.4512 - val_loss: 0.0772 - val_mae: 0.2244\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.3174 - mae: 0.4458 - val_loss: 0.0766 - val_mae: 0.2245\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2780 - mae: 0.4214 - val_loss: 0.0689 - val_mae: 0.2101\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2770 - mae: 0.4195 - val_loss: 0.0671 - val_mae: 0.2071\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2681 - mae: 0.4132 - val_loss: 0.0643 - val_mae: 0.2020\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2561 - mae: 0.3977 - val_loss: 0.0585 - val_mae: 0.1873\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2381 - mae: 0.3845 - val_loss: 0.0569 - val_mae: 0.1873\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2426 - mae: 0.3914 - val_loss: 0.0594 - val_mae: 0.1933\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2222 - mae: 0.3710 - val_loss: 0.0546 - val_mae: 0.1832\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2032 - mae: 0.3556 - val_loss: 0.0535 - val_mae: 0.1816\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2223 - mae: 0.3696 - val_loss: 0.0517 - val_mae: 0.1787\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.2064 - mae: 0.3526 - val_loss: 0.0526 - val_mae: 0.1801\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2167 - mae: 0.3624 - val_loss: 0.0557 - val_mae: 0.1867\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1909 - mae: 0.3429 - val_loss: 0.0508 - val_mae: 0.1761\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1862 - mae: 0.3393 - val_loss: 0.0478 - val_mae: 0.1689\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1871 - mae: 0.3418 - val_loss: 0.0480 - val_mae: 0.1705\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1869 - mae: 0.3412 - val_loss: 0.0484 - val_mae: 0.1723\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1710 - mae: 0.3284 - val_loss: 0.0494 - val_mae: 0.1756\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1844 - mae: 0.3341 - val_loss: 0.0464 - val_mae: 0.1691\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1663 - mae: 0.3212 - val_loss: 0.0455 - val_mae: 0.1671\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1653 - mae: 0.3196 - val_loss: 0.0435 - val_mae: 0.1606\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1750 - mae: 0.3285 - val_loss: 0.0477 - val_mae: 0.1724\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1701 - mae: 0.3222 - val_loss: 0.0427 - val_mae: 0.1610\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1613 - mae: 0.3141 - val_loss: 0.0419 - val_mae: 0.1597\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.1756 - mae: 0.3266 - val_loss: 0.0415 - val_mae: 0.1589\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.1669 - mae: 0.3186 - val_loss: 0.0448 - val_mae: 0.1664\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.1709 - mae: 0.3243 - val_loss: 0.0434 - val_mae: 0.1639\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1700 - mae: 0.3231 - val_loss: 0.0425 - val_mae: 0.1598\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1553 - mae: 0.3084 - val_loss: 0.0414 - val_mae: 0.1587\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1504 - mae: 0.3050 - val_loss: 0.0437 - val_mae: 0.1648\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.1528 - mae: 0.3087 - val_loss: 0.0413 - val_mae: 0.1575\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.1491 - mae: 0.3009 - val_loss: 0.0465 - val_mae: 0.1705\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.1552 - mae: 0.3077 - val_loss: 0.0420 - val_mae: 0.1595\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.1534 - mae: 0.3045 - val_loss: 0.0397 - val_mae: 0.1556\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.1565 - mae: 0.3084 - val_loss: 0.0395 - val_mae: 0.1550\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0395 - mae: 0.1550\n"
     ]
    }
   ],
   "source": [
    "# Experimento 4: Añadir Batch Normalization después de Dropout\n",
    "model_4 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "mae_4, history_4 = train_and_evaluate(model_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Model  Test MAE\n",
      "0    Single Dense Layer  0.162201\n",
      "1    Three Dense Layers  0.179538\n",
      "2               Dropout  0.218859\n",
      "3  Dropout + Batch Norm  0.154970\n"
     ]
    }
   ],
   "source": [
    "# Comparativa de los resultados\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Single Dense Layer', 'Three Dense Layers', 'Dropout', 'Dropout + Batch Norm'],\n",
    "    'Test MAE': [mae_1, mae_2, mae_3, mae_4]\n",
    "})\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo con Dropout + Batch Normalization tiene el menor Test MAE de 0.154970, lo que significa que este es el modelo más preciso de los cuatro en predecir el GPA de los estudiantes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
